
## Project Architecture Overview

This scraping hub implements a modern ETL (Extract, Transform, Load) architecture with two primary data acquisition methods:

1. **API Requests + TLS Fingerprint Masking** (90% of use cases)
2. **Browser Automation** (10% fallback for complex sites)

### Core Philosophy
- **Modularity First**: Every component should be reusable across projects
- **Anti-Detection**: Use cutting-edge techniques to avoid blocking
- **ETL Separation**: Clear boundaries between extraction, transformation, and loading
- **Graceful Degradation**: Try API first, fallback to browser automation

## Technical Stack Rationale

### HTTP Clients (Priority Order)
1. **curl_cffi** - Primary choice for TLS fingerprint masking
   - Mimics real browser TLS handshakes
   - Supports HTTP/2 and modern protocols
   - Built on libcurl for reliability
   
2. **tls_client** - Backup TLS masking solution
   - Alternative implementation for fingerprint spoofing
   - Good for specific edge cases where curl_cffi fails

3. **nodriver** - Primary browser automation
   - Undetected Chrome automation
   - Built-in stealth capabilities
   - Async-first design
   - Successor to undetected-chromedriver

4. **playwright** - Fallback browser automation
   - Cross-browser support
   - Rich feature set for complex interactions
   - Requires stealth plugins for anti-detection

### Data Processing Stack
- **selectolax** - Fast HTML parsing (10x faster than BeautifulSoup)
- **pydantic** - Data validation and modeling
- **pandas** - Data manipulation and analysis
- **loguru** - Advanced logging with rotation and filtering
- **tenacity** - Retry logic with exponential backoff

## Common Scraping Patterns

### Pattern 1: API Discovery Workflow
```
1. Load target website in browser dev tools
2. Monitor Network → Fetch/XHR tab
3. Identify JSON API endpoints
4. Copy requests as curl commands
5. Convert to curl_cffi implementation
6. Extract structured JSON data
```

### Pattern 2: Browser Automation Workflow
```
1. Use nodriver for stealth navigation
2. Wait for dynamic content loading
3. Extract data using CSS selectors
4. Handle pagination and infinite scroll
5. Implement proper cleanup
```

### Pattern 3: Hybrid Approach
```
1. Use browser to discover API endpoints
2. Extract session cookies/tokens
3. Switch to curl_cffi for bulk data extraction
4. Combine browser state with API efficiency
```

## ETL Component Responsibilities

### Extractor Classes
**Purpose**: Acquire raw data from target sources
**Responsibilities**:
- Handle authentication and session management
- Implement retry logic for failed requests
- Manage proxy rotation and user agent switching
- Return standardized response objects
- Log all network activity

**Key Methods**:
- `extract_data(url: str) -> Dict[str, Any]`
- `setup_session() -> None`
- `handle_errors(exception: Exception) -> None`

### Transformer Classes
**Purpose**: Convert raw data into structured formats
**Responsibilities**:
- Parse HTML/JSON using selectolax or built-in JSON
- Validate data using Pydantic models
- Handle missing or malformed data gracefully
- Apply business logic transformations
- Generate clean, consistent output

**Key Methods**:
- `transform(raw_data: str) -> List[BaseModel]`
- `validate_item(item: Dict) -> Optional[BaseModel]`
- `clean_text(text: str) -> str`

### Loader Classes
**Purpose**: Persist processed data to storage systems
**Responsibilities**:
- Support multiple output formats (JSON, CSV, SQLite, PostgreSQL)
- Handle file naming with timestamps
- Implement deduplication logic
- Manage batch operations for performance
- Ensure data integrity

**Key Methods**:
- `load(data: List[Dict], destination: str) -> None`
- `setup_storage() -> None`
- `deduplicate(data: List[Dict]) -> List[Dict]`

## Anti-Detection Strategies

### TLS Fingerprinting
- Always use curl_cffi with `impersonate` parameter
- Rotate between different browser profiles
- Match HTTP/2 settings to target browser
- Avoid obviously automated request patterns

### Browser Fingerprinting
- Use nodriver's built-in stealth mode
- Implement realistic viewport sizes (1920x1080)
- Add random delays between actions (2-5 seconds)
- Block unnecessary resources (images, ads, fonts)

### Behavioral Patterns
- Respect robots.txt when legally required
- Implement realistic request timing
- Use residential proxy rotation
- Maintain consistent session behavior

## Error Handling Philosophy

### Exception Hierarchy
```python
ScrapingError (base)
├── NetworkError
│   ├── ConnectionTimeout
│   ├── HTTPError
│   └── ProxyError
├── ParsingError
│   ├── InvalidHTML
│   ├── MissingSelector
│   └── DataValidationError
└── BrowserError
    ├── ElementNotFound
    ├── PageLoadTimeout
    └── JavaScriptError
```

### Retry Strategy
- Use tenacity for exponential backoff
- Different retry counts for different error types
- Log all retry attempts with context
- Fail fast for permanent errors (404, 403)
- Circuit breaker pattern for cascading failures

## Performance Optimization

### Async Patterns
- Use `asyncio` for concurrent HTTP requests
- Implement semaphores to limit concurrent connections
- Use `aiofiles` for non-blocking file operations
- Batch database operations

### Memory Management
- Process data in chunks for large datasets
- Use generators instead of loading everything into memory
- Properly close browser instances and HTTP sessions
- Monitor memory usage in long-running processes

### Caching Strategies
- Cache successful responses to avoid re-scraping
- Store session cookies for authenticated endpoints
- Implement TTL-based cache invalidation
- Use Redis for distributed caching